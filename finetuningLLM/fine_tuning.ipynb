{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import jsonlines\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to split a text into chunks of a specified maximum token length\n",
    "def chunk_text(text, max_tokens):\n",
    "    chunks = []\n",
    "    words = text.split()\n",
    "    current_chunk = \"\"\n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) < max_tokens:\n",
    "            current_chunk += word + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word + \" \"\n",
    "    chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pdf_folder = '../papers'  # Adjust the folder path to the location of your PDFs\n",
    "output_file = 'training_data.jsonl'  # Output JSONL file path\n",
    "\n",
    "max_tokens_per_chunk = 2048  # Adjust this value based on your model's token limit\n",
    "\n",
    "# Iterate over PDF files in the folder\n",
    "pdf_files = [file for file in os.listdir(pdf_folder) if file.endswith('.pdf')]\n",
    "\n",
    "# Convert each PDF to text, chunk the text, and write to the JSONL file\n",
    "with jsonlines.open(output_file, mode='w') as writer:\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(pdf_folder, pdf_file)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        text_chunks = chunk_text(text, max_tokens_per_chunk)\n",
    "\n",
    "        # Iterate over the chunks and save them as separate training examples\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            training_example = {\n",
    "                \"prompt\": chunk,\n",
    "                \"completion\": \"\"  # Add the ideal generated text if available\n",
    "            }\n",
    "            writer.write(training_example)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we've created the data in the format (prepared in a JSONL) that we can process it through openai fine-tuning model\n",
    "\n",
    "we can use the following commands"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. export OPENAI_API_KEY=\"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\"\n",
    "2. openai api fine_tunes.create -t training_data.jsonl -m davinci\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai api fine_tunes.follow -i ft-u2Zndb98jHZ7NY6XxfGXiOsA\n",
    "openai api fine_tunes.get -i ft-u2Zndb98jHZ7NY6XxfGXiOsA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
