{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PyPDF2\n",
    "from transformers import GPT2TokenizerFast\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    return {rowIndex: get_embedding(row.content) for rowIndex, row in df.iterrows()}\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = sorted([\n",
    "        (np.dot(np.array(query_embedding), np.array(doc_embedding)), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    return document_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m similar_ones \u001b[39m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m query_chunk \u001b[39min\u001b[39;00m query_chunks:\n\u001b[1;32m---> 23\u001b[0m     similar_ones \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(similar_ones)\n",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m, in \u001b[0;36mcompute_doc_embeddings\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39;49mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text: \u001b[39mstr\u001b[39m, model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m---> 31\u001b[0m     result \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     32\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtext\n\u001b[0;32m     33\u001b[0m     )\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"What type of mutations are there in transgenic mice?\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)\n",
    "\n",
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "print(similar_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    embeddings = {}\n",
    "    for rowIndex, row in df.iterrows():\n",
    "        embeddings[rowIndex] = []\n",
    "        for chunk in row.chunks:\n",
    "            embeddings[rowIndex].append(get_embedding(chunk))\n",
    "    return embeddings\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = []\n",
    "    for doc_index, doc_embeddings in contexts.items():\n",
    "        max_similarity = -1  # Initialize max_similarity with a negative value\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(np.array(query_embedding), np.array(doc_embedding))\n",
    "            max_similarity = max(max_similarity, similarity)  # Update max_similarity if the current similarity is higher\n",
    "        document_similarities.append((max_similarity, doc_index))\n",
    "    document_similarities.sort(reverse=True)  # Sort the list in descending order of similarity scores\n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7201144051191797, 9), (0.7198310961928849, 0), (0.6979709709794365, 4), (0.695654594216248, 7), (0.6937435156971158, 5), (0.6920539139069725, 3), (0.6905764225246718, 8), (0.6901786153247469, 1), (0.6802488024393113, 2), (0.6777508786938459, 6)]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"papers\"  \n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"What type of mutations are there in transgenic mice?\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)\n",
    "\n",
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "print(similar_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    embeddings = {}\n",
    "    for rowIndex, row in df.iterrows():\n",
    "        embeddings[rowIndex] = []\n",
    "        for chunk in row.chunks:\n",
    "            embeddings[rowIndex].append(get_embedding(chunk))\n",
    "    return embeddings\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = []\n",
    "    for doc_index, doc_embeddings in contexts.items():\n",
    "        max_similarity = -1  # Initialize max_similarity with a negative value\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(np.array(query_embedding), np.array(doc_embedding))\n",
    "            max_similarity = max(max_similarity, similarity)  # Update max_similarity if the current similarity is higher\n",
    "        document_similarities.append((max_similarity, doc_index))\n",
    "    document_similarities.sort(reverse=True)  # Sort the list in descending order of similarity scores\n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 73117 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m:user_text})\n\u001b[0;32m     33\u001b[0m \u001b[39m# Generate a response using the GPT-3.5-turbo model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m'\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages)\n\u001b[0;32m     35\u001b[0m reply \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(reply)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 73117 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"What type of mutations are there in transgenic mice?\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)\n",
    "\n",
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "# Add the conversation system message\n",
    "messages = [{'role':'system', 'content':'You are a professor who provides to the point answers'}]\n",
    "\n",
    "# Get the content of the most similar document\n",
    "user_text = df['content'][similar_ones[0][1]]\n",
    "user_text = f\"How poverty is the reason for homelessness? \\n {user_text}\"\n",
    "messages.append({'role':'user', 'content':user_text})\n",
    "\n",
    "# Generate a response using the GPT-3.5-turbo model\n",
    "response = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    embeddings = {}\n",
    "    for rowIndex, row in df.iterrows():\n",
    "        embeddings[rowIndex] = []\n",
    "        for chunk in row.chunks:\n",
    "            embeddings[rowIndex].append(get_embedding(chunk))\n",
    "    return embeddings\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = []\n",
    "    for doc_index, doc_embeddings in contexts.items():\n",
    "        max_similarity = -1  # Initialize max_similarity with a negative value\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(np.array(query_embedding), np.array(doc_embedding))\n",
    "            max_similarity = max(max_similarity, similarity)  # Update max_similarity if the current similarity is higher\n",
    "        document_similarities.append((max_similarity, doc_index))\n",
    "    document_similarities.sort(reverse=True)  # Sort the list in descending order of similarity scores\n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    embeddings = {}\n",
    "    for rowIndex, row in df.iterrows():\n",
    "        embeddings[rowIndex] = []\n",
    "        for chunk in row.chunks:\n",
    "            embeddings[rowIndex].append(get_embedding(chunk))\n",
    "    return embeddings\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = []\n",
    "    for doc_index, doc_embeddings in contexts.items():\n",
    "        max_similarity = -1  # Initialize max_similarity with a negative value\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(np.array(query_embedding), np.array(doc_embedding))\n",
    "            max_similarity = max(max_similarity, similarity)  # Update max_similarity if the current similarity is higher\n",
    "        document_similarities.append((max_similarity, doc_index))\n",
    "    document_similarities.sort(reverse=True)  # Sort the list in descending order of similarity scores\n",
    "    return document_similarities\n",
    "\n",
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"What type of mutations are there in transgenic mice?\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'return_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m system_message_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mencode(messages[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m     14\u001b[0m max_user_text_length \u001b[39m=\u001b[39m \u001b[39m4097\u001b[39m \u001b[39m-\u001b[39m system_message_length\n\u001b[1;32m---> 16\u001b[0m user_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(user_text, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mmax_user_text_length, return_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     18\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m:user_text})\n\u001b[0;32m     20\u001b[0m \u001b[39m# Generate a response using the GPT-3.5-turbo model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2319\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2283\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2302\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2303\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m   2304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2305\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2306\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2317\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[0;32m   2318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2320\u001b[0m         text,\n\u001b[0;32m   2321\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2322\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2323\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2324\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2325\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2326\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2327\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2328\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2329\u001b[0m     )\n\u001b[0;32m   2331\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2727\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2717\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2718\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2719\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2720\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2724\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2725\u001b[0m )\n\u001b[1;32m-> 2727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2728\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2729\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2730\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2731\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2732\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2733\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2734\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2735\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2736\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2737\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2738\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2739\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2740\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2741\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2742\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2743\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2744\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2745\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2746\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2_fast.py:178\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    173\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[0;32m    174\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m )\n\u001b[1;32m--> 178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_encode_plus(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[0;32m    479\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    480\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    498\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[0;32m    499\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[1;32m--> 500\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    501\u001b[0m         batched_input,\n\u001b[0;32m    502\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m    503\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m    504\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    505\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    506\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    507\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m    508\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    509\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m    510\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    511\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    512\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    513\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    514\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    515\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m    516\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    517\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    520\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    521\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2_fast.py:168\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[0;32m    164\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m )\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_batch_encode_plus(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'return_text'"
     ]
    }
   ],
   "source": [
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "# Add the conversation system message\n",
    "messages = [{'role':'system', 'content':'You are a professor who provides to the point answers'}]\n",
    "\n",
    "# Get the content of the most similar document\n",
    "user_text = df['content'][similar_ones[0][1]]\n",
    "user_text = f\"How poverty is the reason for homelessness? \\n {user_text}\"\n",
    "\n",
    "# Make sure the total tokens for user_text + initial message is within model's max limit (4097 tokens)\n",
    "system_message_length = len(tokenizer.encode(messages[0]['content']))\n",
    "max_user_text_length = 4097 - system_message_length\n",
    "\n",
    "user_text = tokenizer.encode(user_text, truncation=True, max_length=max_user_text_length, return_text=True)\n",
    "\n",
    "messages.append({'role':'user', 'content':user_text})\n",
    "\n",
    "# Generate a response using the GPT-3.5-turbo model\n",
    "response = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    embeddings = {}\n",
    "    for rowIndex, row in df.iterrows():\n",
    "        embeddings[rowIndex] = []\n",
    "        for chunk in row.chunks:\n",
    "            embeddings[rowIndex].append(get_embedding(chunk))\n",
    "    return embeddings\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = []\n",
    "    for doc_index, doc_embeddings in contexts.items():\n",
    "        max_similarity = -1  # Initialize max_similarity with a negative value\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(np.array(query_embedding), np.array(doc_embedding))\n",
    "            max_similarity = max(max_similarity, similarity)  # Update max_similarity if the current similarity is higher\n",
    "        document_similarities.append((max_similarity, doc_index))\n",
    "    document_similarities.sort(reverse=True)  # Sort the list in descending order of similarity scores\n",
    "    return document_similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Experiment:\n",
      "\n",
      "Title: Investigating the Combined Effects of Gene Knockouts on Adaptive Thermogenesis in Adult Mice using BAd-CRISPR Methodology\n",
      "\n",
      "Hypothesis: Simultaneous knockout of multiple genes involved in brown adipose tissue (BAT) regulation and metabolism will result in significant alterations in adaptive thermogenesis and overall energy homeostasis in adult mice.\n",
      "\n",
      "Objective: To evaluate the combined effects of simultaneous knockouts of selected genes (Adipoq, Atgl, Fasn, Plin1, Scd1, and Ucp1) in BAT of adult mice using BAd-CRISPR and assess the impact on adaptive thermogenesis, energy expenditure, and whole-body metabolism.\n",
      "\n",
      "Methods:\n",
      "\n",
      "1. Generate mouse models with brown adipocyte-specific Cas9 expression.\n",
      "\n",
      "2. Design sgRNAs targeting the six selected genes (Adipoq, Atgl, Fasn, Plin1, Scd1, and Ucp1) using the CRISPOR design tool and cloned into the AAV8-sgRNA vector.\n",
      "\n",
      "3. Create different combinations of AAV8-sgRNA for co-administration, targeting multiple genes in each group.\n",
      "\n",
      "4. Administer the AAV8-sgRNAs to interscapular BAT of adult mice expressing Cas9 in brown adipocytes using BAd-CRISPR methodology.\n",
      "\n",
      "5. Validate gene knockout efficiency by analyzing mRNA and protein expression of the targeted genes in BAT.\n",
      "\n",
      "6. Characterize the phenotypic effects of simultaneous gene knockouts in terms of adaptive thermogenesis, cold tolerance, energy expenditure, and metabolic changes using various techniques, including indirect calorimetry, measurement of body temperature, histological analysis of BAT, and assessment of circulating factors.\n",
      "\n",
      "7. Analyze the transcriptomic and proteomic changes in the BAT of the experimental mice.\n",
      "\n",
      "Expected Outcomes:\n",
      "\n",
      "1. Confirmation of the efficiency of BAd-CRISPR in knocking out multiple genes simultaneously in BAT of adult mice.\n",
      "\n",
      "2. Determination of the combined effects of simultaneous gene knockouts on BAT function, adaptive thermogenesis, and whole-body metabolism in adult mice.\n",
      "\n",
      "3. Identification of potential compensatory mechanisms in response to the knockout of multiple genes in BAT.\n",
      "\n",
      "4. Advancement in our understanding of the functional roles of these genes in BAT biology and the discovery of potential therapeutic targets for obesity and other metabolic disorders.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"Use these 10 papers to propose an experiment. They are cross-disciplinary papers and you can use them to propose an experiment in this field.\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)\n",
    "\n",
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "# Add the conversation system message\n",
    "messages = [{'role':'system', 'content':'You are a professor who provides to the point answers'}]\n",
    "\n",
    "# Get the content of the most similar document\n",
    "user_text = df['content'][similar_ones[0][1]]\n",
    "user_text = f\"propose an experiment based on these 10 papers? \\n {user_text}\"\n",
    "\n",
    "# Make sure the total tokens for user_text + initial message is within model's max limit (4097 tokens)\n",
    "system_message_length = len(tokenizer.encode(messages[0]['content']))\n",
    "max_user_text_length = 4097 - system_message_length\n",
    "\n",
    "#truncate the tokens\n",
    "user_tokens = tokenizer.encode(user_text, truncation=True, max_length=max_user_text_length)\n",
    "\n",
    "# Convert the truncated tokens back into text\n",
    "user_text = tokenizer.decode(user_tokens)\n",
    "\n",
    "messages.append({'role':'user', 'content':user_text})\n",
    "\n",
    "# Generate a response using the GPT-4 model\n",
    "response = openai.ChatCompletion.create(model='gpt-4', messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yc_company",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
