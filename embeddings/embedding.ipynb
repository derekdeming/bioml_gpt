{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import tiktoken\n",
    "import openai \n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key  = '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'\n",
    "encoding = tiktoken.get_encoding('cl100k_base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.6/232.6 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            fileName                                            content   \n",
      "0       db201001.pdf  Adipocyte-Specific Deletion of Lamin A/C Large...  \\\n",
      "1      dbi200015.pdf  Wnt Signaling: From Mesenchymal Cell Fate to\\n...   \n",
      "2    elife-78496.pdf  Li et al. eLife 2022;11:e78496. DOI: https://d...   \n",
      "3           main.pdf  Wnt/b-catenin signaling regulates adipose tiss...   \n",
      "4         main_1.pdf  BAd-CRISPR: Inducible gene knockout in intersc...   \n",
      "5         main_2.pdf  Wntless regulates lipogenic gene expression in...   \n",
      "6  nihms-1699598.pdf  Preclinical models for investigating how bone ...   \n",
      "7   nihms-611884.pdf  Bone marrow adipose tissue is an endocrine org...   \n",
      "8    nihms941200.pdf  Development, regulation, metabolism and functi...   \n",
      "9   pbio.3000988.pdf  ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wtm...   \n",
      "\n",
      "   tokens  \n",
      "0   15092  \n",
      "1   13207  \n",
      "2   34301  \n",
      "3   31138  \n",
      "4   26872  \n",
      "5   29683  \n",
      "6   16538  \n",
      "7   11268  \n",
      "8   14120  \n",
      "9   73085  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "folder_path = \"../papers\"  # path of folder \n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        tokens = encoding.encode(' '.join(pdf_contents))\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': ' '.join(pdf_contents), 'tokens': len(tokens)}\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15423 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\tokenization_utils_base.py:249\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m    250\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'overflowing_tokens'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m         content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(pdf_contents)\n\u001b[0;32m     33\u001b[0m         tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(content)\n\u001b[1;32m---> 34\u001b[0m         chunks \u001b[39m=\u001b[39m split_into_chunks(content, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m         df\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(df)] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfileName\u001b[39m\u001b[39m'\u001b[39m: file_name, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: content, \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(tokens), \u001b[39m'\u001b[39m\u001b[39mchunks\u001b[39m\u001b[39m'\u001b[39m: chunks}\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36msplit_into_chunks\u001b[1;34m(text, max_tokens)\u001b[0m\n\u001b[0;32m     15\u001b[0m chunks \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode_plus(text, max_length\u001b[39m=\u001b[39mmax_tokens, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_overflowing_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m \u001b[39mwhile\u001b[39;00m encoding\u001b[39m.\u001b[39;49moverflowing_tokens:\n\u001b[0;32m     18\u001b[0m     chunk \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39mtokens[:max_tokens]\n\u001b[0;32m     19\u001b[0m     chunks\u001b[39m.\u001b[39mappend(tokenizer\u001b[39m.\u001b[39mdecode(chunk))\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\transformers\\tokenization_utils_base.py:251\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m    250\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import PyPDF2\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=500):\n",
    "    chunks = []\n",
    "    encoding = tokenizer.encode_plus(text, max_length=max_tokens, truncation=True, return_overflowing_tokens=True)\n",
    "    while encoding.overflowing_tokens:\n",
    "        chunk = encoding.tokens[:max_tokens]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        encoding = tokenizer.encode_plus(tokenizer.decode(encoding.overflowing_tokens), max_length=max_tokens, truncation=True, return_overflowing_tokens=True)\n",
    "    chunks.append(tokenizer.decode(encoding.input_ids))\n",
    "    return chunks\n",
    "\n",
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content)\n",
    "        chunks = split_into_chunks(content, max_tokens=500)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15423 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>db201001.pdf</td>\n",
       "      <td>Adipocyte-Specific Deletion of Lamin A/C Large...</td>\n",
       "      <td>15423</td>\n",
       "      <td>[Adipocyte-Specific Deletion of Lamin A/C Larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbi200015.pdf</td>\n",
       "      <td>Wnt Signaling: From Mesenchymal Cell Fate to\\n...</td>\n",
       "      <td>13805</td>\n",
       "      <td>[Wnt Signaling: From Mesenchymal Cell Fate to\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elife-78496.pdf</td>\n",
       "      <td>Li et al. eLife 2022;11:e78496. DOI: https://d...</td>\n",
       "      <td>36364</td>\n",
       "      <td>[Li et al. eLife 2022;11:e78496. DOI: https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.pdf</td>\n",
       "      <td>Wnt/b-catenin signaling regulates adipose tiss...</td>\n",
       "      <td>32788</td>\n",
       "      <td>[Wnt/b-catenin signaling regulates adipose tis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main_1.pdf</td>\n",
       "      <td>BAd-CRISPR: Inducible gene knockout in intersc...</td>\n",
       "      <td>27587</td>\n",
       "      <td>[BAd-CRISPR: Inducible gene knockout in inters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>main_2.pdf</td>\n",
       "      <td>Wntless regulates lipogenic gene expression in...</td>\n",
       "      <td>30809</td>\n",
       "      <td>[Wntless regulates lipogenic gene expression i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nihms-1699598.pdf</td>\n",
       "      <td>Preclinical models for investigating how bone ...</td>\n",
       "      <td>17036</td>\n",
       "      <td>[Preclinical models for investigating how bone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nihms-611884.pdf</td>\n",
       "      <td>Bone marrow adipose tissue is an endocrine org...</td>\n",
       "      <td>11363</td>\n",
       "      <td>[Bone marrow adipose tissue is an endocrine or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nihms941200.pdf</td>\n",
       "      <td>Development, regulation, metabolism and functi...</td>\n",
       "      <td>14558</td>\n",
       "      <td>[Development, regulation, metabolism and funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pbio.3000988.pdf</td>\n",
       "      <td>ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wtm...</td>\n",
       "      <td>77161</td>\n",
       "      <td>[ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fileName                                            content   \n",
       "0       db201001.pdf  Adipocyte-Specific Deletion of Lamin A/C Large...  \\\n",
       "1      dbi200015.pdf  Wnt Signaling: From Mesenchymal Cell Fate to\\n...   \n",
       "2    elife-78496.pdf  Li et al. eLife 2022;11:e78496. DOI: https://d...   \n",
       "3           main.pdf  Wnt/b-catenin signaling regulates adipose tiss...   \n",
       "4         main_1.pdf  BAd-CRISPR: Inducible gene knockout in intersc...   \n",
       "5         main_2.pdf  Wntless regulates lipogenic gene expression in...   \n",
       "6  nihms-1699598.pdf  Preclinical models for investigating how bone ...   \n",
       "7   nihms-611884.pdf  Bone marrow adipose tissue is an endocrine org...   \n",
       "8    nihms941200.pdf  Development, regulation, metabolism and functi...   \n",
       "9   pbio.3000988.pdf  ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wtm...   \n",
       "\n",
       "   tokens                                             chunks  \n",
       "0   15423  [Adipocyte-Specific Deletion of Lamin A/C Larg...  \n",
       "1   13805  [Wnt Signaling: From Mesenchymal Cell Fate to\\...  \n",
       "2   36364  [Li et al. eLife 2022;11:e78496. DOI: https://...  \n",
       "3   32788  [Wnt/b-catenin signaling regulates adipose tis...  \n",
       "4   27587  [BAd-CRISPR: Inducible gene knockout in inters...  \n",
       "5   30809  [Wntless regulates lipogenic gene expression i...  \n",
       "6   17036  [Preclinical models for investigating how bone...  \n",
       "7   11363  [Bone marrow adipose tissue is an endocrine or...  \n",
       "8   14558  [Development, regulation, metabolism and funct...  \n",
       "9   77161  [ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wt...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import PyPDF2\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=500):\n",
    "    chunks = []\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "    start = 0\n",
    "    while start < len(tokenized_text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(tokenized_text):\n",
    "            end = len(tokenized_text)\n",
    "        chunk = tokenized_text[start:end]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "folder_path = \"papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 1024  # Set maximum sequence length for the model\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content)\n",
    "        chunks = split_into_chunks(content, max_tokens=500)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>db201001.pdf</td>\n",
       "      <td>Adipocyte-Specific Deletion of Lamin A/C Large...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[2782, 541, 43320, 12, 32419, 1024, 1616, 295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbi200015.pdf</td>\n",
       "      <td>Wnt Signaling: From Mesenchymal Cell Fate to\\n...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[54, 429, 5865, 4272, 25, 3574, 14937, 24421,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elife-78496.pdf</td>\n",
       "      <td>Li et al. eLife 2022;11:e78496. DOI: https://d...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[32304, 2123, 1849, 282, 13, 304, 14662, 3316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.pdf</td>\n",
       "      <td>Wnt/b-catenin signaling regulates adipose tiss...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[54, 429, 14, 65, 12, 9246, 268, 259, 22049, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main_1.pdf</td>\n",
       "      <td>BAd-CRISPR: Inducible gene knockout in intersc...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[33, 2782, 12, 9419, 1797, 4805, 25, 1423, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>main_2.pdf</td>\n",
       "      <td>Wntless regulates lipogenic gene expression in...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[54, 429, 1203, 39474, 10645, 15147, 9779, 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nihms-1699598.pdf</td>\n",
       "      <td>Preclinical models for investigating how bone ...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[6719, 47367, 4981, 329, 10240, 703, 9970, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nihms-611884.pdf</td>\n",
       "      <td>Bone marrow adipose tissue is an endocrine org...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[49580, 44173, 31659, 577, 10712, 318, 281, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nihms941200.pdf</td>\n",
       "      <td>Development, regulation, metabolism and functi...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[41206, 11, 9001, 11, 20211, 290, 2163, 286, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pbio.3000988.pdf</td>\n",
       "      <td>ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wtm...</td>\n",
       "      <td>1024</td>\n",
       "      <td>[[57, 42, 58, 42, 37, 1168, 1268, 376, 57, 59,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fileName                                            content   \n",
       "0       db201001.pdf  Adipocyte-Specific Deletion of Lamin A/C Large...  \\\n",
       "1      dbi200015.pdf  Wnt Signaling: From Mesenchymal Cell Fate to\\n...   \n",
       "2    elife-78496.pdf  Li et al. eLife 2022;11:e78496. DOI: https://d...   \n",
       "3           main.pdf  Wnt/b-catenin signaling regulates adipose tiss...   \n",
       "4         main_1.pdf  BAd-CRISPR: Inducible gene knockout in intersc...   \n",
       "5         main_2.pdf  Wntless regulates lipogenic gene expression in...   \n",
       "6  nihms-1699598.pdf  Preclinical models for investigating how bone ...   \n",
       "7   nihms-611884.pdf  Bone marrow adipose tissue is an endocrine org...   \n",
       "8    nihms941200.pdf  Development, regulation, metabolism and functi...   \n",
       "9   pbio.3000988.pdf  ZK[KF ZIN FZ\\OIS K\\n\\soy{womuwk~ kzn yotkl{wtm...   \n",
       "\n",
       "   tokens                                             chunks  \n",
       "0    1024  [[2782, 541, 43320, 12, 32419, 1024, 1616, 295...  \n",
       "1    1024  [[54, 429, 5865, 4272, 25, 3574, 14937, 24421,...  \n",
       "2    1024  [[32304, 2123, 1849, 282, 13, 304, 14662, 3316...  \n",
       "3    1024  [[54, 429, 14, 65, 12, 9246, 268, 259, 22049, ...  \n",
       "4    1024  [[33, 2782, 12, 9419, 1797, 4805, 25, 1423, 12...  \n",
       "5    1024  [[54, 429, 1203, 39474, 10645, 15147, 9779, 54...  \n",
       "6    1024  [[6719, 47367, 4981, 329, 10240, 703, 9970, 44...  \n",
       "7    1024  [[49580, 44173, 31659, 577, 10712, 318, 281, 8...  \n",
       "8    1024  [[41206, 11, 9001, 11, 20211, 290, 2163, 286, ...  \n",
       "9    1024  [[57, 42, 58, 42, 37, 1168, 1268, 376, 57, 59,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=500):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "folder_path = \"../papers\"  # the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 1024  # Set maximum sequence length for the model\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=tokenizer.model_max_length)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=500)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text:str, model:str=\"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_embeddings(df:pd.DataFrame) -> dict[tuple[str,str], list[float]]:\n",
    "    return {rowIndex: get_embedding(row.content) for rowIndex, row in df.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str,str), np.array]) -> list[(float, (str,str))]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = sorted([\n",
    "        (np.dot(np.array(query_embedding), np.array(doc_embedding)), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m similar_ones\u001b[39m=\u001b[39morder_document_sections_by_query_similarity(\u001b[39m\"\u001b[39m\u001b[39mWhat type of mutations are there in transgenic mice?\u001b[39m\u001b[39m\"\u001b[39m, compute_doc_embeddings(df))\n",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m, in \u001b[0;36mcompute_doc_embeddings\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df:pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m,\u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df:pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m,\u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39;49mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text:\u001b[39mstr\u001b[39m, model:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     result \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      3\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtext\n\u001b[0;32m      4\u001b[0m     )\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "similar_ones=order_document_sections_by_query_similarity(\"What type of mutations are there in transgenic mice?\", compute_doc_embeddings(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PyPDF2\n",
    "from transformers import GPT2TokenizerFast\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf_contents = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_contents.append(page.extract_text())\n",
    "    return pdf_contents\n",
    "\n",
    "def split_into_chunks(text, max_tokens=8191):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_tokens\n",
    "        if end > len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return result['data'][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    return {rowIndex: get_embedding(row.content) for rowIndex, row in df.iterrows()}\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> list[(float, tuple[str, str])]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = sorted([\n",
    "        (np.dot(np.array(query_embedding), np.array(doc_embedding)), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    return document_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m similar_ones \u001b[39m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m query_chunk \u001b[39min\u001b[39;00m query_chunks:\n\u001b[1;32m---> 23\u001b[0m     similar_ones \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(similar_ones)\n",
      "Cell \u001b[1;32mIn[30], line 37\u001b[0m, in \u001b[0;36mcompute_doc_embeddings\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[30], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m {rowIndex: get_embedding(row\u001b[39m.\u001b[39;49mcontent) \u001b[39mfor\u001b[39;00m rowIndex, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()}\n",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text: \u001b[39mstr\u001b[39m, model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m---> 31\u001b[0m     result \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     32\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtext\n\u001b[0;32m     33\u001b[0m     )\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\anaconda3\\envs\\yc_company\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 8191 tokens, however you requested 15092 tokens (15092 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "folder_path = \"../papers\"  # Replace with the actual folder path\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 8191  # Set maximum sequence length for the model\n",
    "\n",
    "df = pd.DataFrame(columns=['fileName', 'content', 'tokens', 'chunks'])\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        pdf_contents = read_pdf(file_path)\n",
    "        content = ' '.join(pdf_contents)\n",
    "        tokens = tokenizer.encode(content, truncation=True, max_length=8191)\n",
    "        chunks = split_into_chunks(tokens, max_tokens=8191)\n",
    "        df.loc[len(df)] = {'fileName': file_name, 'content': content, 'tokens': len(tokens), 'chunks': chunks}\n",
    "\n",
    "query = \"What type of mutations are there in transgenic mice?\"\n",
    "\n",
    "# Split the query into smaller chunks\n",
    "query_chunks = split_into_chunks(query, max_tokens=8191)\n",
    "\n",
    "similar_ones = []\n",
    "for query_chunk in query_chunks:\n",
    "    similar_ones += order_document_sections_by_query_similarity(query_chunk, compute_doc_embeddings(df))\n",
    "\n",
    "print(similar_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yc_company",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
